{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/josipbencic/kaggle/blob/master/predict_future_sales_josip.ipynb",
      "authorship_tag": "ABX9TyP8wIk8siC3uykIi8joKxf8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josipbencic/kaggle/blob/master/predict_future_sales_josip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo2Ssr0VHt2P",
        "colab_type": "text"
      },
      "source": [
        "### 0 Technical boilerplate to make the notebook work\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWaWo5-YHuR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "ROOT = \"/content/drive\"     # default location for the drive\n",
        "PROJECT_PATH = \"/content/drive/My Drive/kaggle_tmp\"\n",
        "\n",
        "drive.mount(ROOT)           # we mount the google drive at /content/drive\n",
        "\n",
        "GIT_TOKEN = \"13b32b3481c45a9a7bdd7f223644aa47b1156c42\"\n",
        "GIT_USERNAME = \"josipbencic\"\n",
        "GIT_REPOSITORY = \"kaggle\"\n",
        "\n",
        "GIT_PATH = \"https://\" + GIT_TOKEN + \"@github.com/\" + GIT_USERNAME + \"/\" + GIT_REPOSITORY + \".git\"\n",
        "\n",
        "!git clone \"{GIT_PATH}\" ./temp      # clone github repository to temp folder\n",
        "!cp -R ./temp/* \"{PROJECT_PATH}\"    # move all files/folders in temp folder to folder defined in project path\n",
        "!rm -rf ./temp                      # remove all the files/folders in temp folder\n",
        "#!rsync -aP --exclude=data/ \"{PROJECT_PATH}\"/*  ./   # use remote sync to copy from google drive to local runtime google colab\n",
        "                                                     # but exclude data folder                                          \n",
        "!rsync -aP \"{PROJECT_PATH}\"/* ./\n",
        "\n",
        "!pip install category_encoders\n",
        "\n",
        "## Inspect collab machine\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "!pip install pycuda\n",
        "!pip install scikit-cuda\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVWwMdXIFOiu",
        "colab_type": "text"
      },
      "source": [
        "### 1 Thoughts, ideas, conclusions\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk16iBxWIF8d",
        "colab_type": "text"
      },
      "source": [
        "Ideas\n",
        "\n",
        "- aggregate items per category instead of per item_id\n",
        "   - there are only ~80 categories, while 20k items\n",
        "   - this will reduce the dimensionality in the final version and make\n",
        "       the model more interpretable, albeit less accurate\n",
        "   - in that case, what to do with the prices!!??\n",
        "      - Around 15.5k of items have changed their price over time, ~70%\n",
        "   - can do 2 versions and then cross validate:\n",
        "       - with averaged and with summed volumes per item\n",
        "   - the alternative is to use this column as a latent variable\n",
        "- battle non-stationarity by building a model only with items that are\n",
        "   still being sold\n",
        "- add item price and volume history in the past months as columns\n",
        "- add a column that asks if the item existed a year ago\n",
        "- add a column that says how many items of such were sold last year\n",
        "   in that month\n",
        "- add last month's sales\n",
        "- add a few last months' price\n",
        "- try to backtrade the model to optimize for the parameters\n",
        "\n",
        "Conclusions\n",
        "- it's better to model with logarithm of prices rather than with the price itself\n",
        "  - actually, prices are fucking up everything\n",
        "  - except the current price, that's useful\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVhPGfSRH3ea",
        "colab_type": "text"
      },
      "source": [
        "### 2 Includes and data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0HMYAyDIcQo",
        "colab_type": "code",
        "outputId": "11e28729-d55c-4298-e6e2-0e2f8c426216",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "from typing import List, Set, Dict, Tuple, Optional\n",
        "import argparse\n",
        "import sys\n",
        "import os\n",
        "import csv\n",
        "import time\n",
        "import math\n",
        "from datetime import date\n",
        "from math import floor\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from pandas.plotting import register_matplotlib_converters\n",
        "register_matplotlib_converters()\n",
        "\n",
        "import category_encoders as ce\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tools.eval_measures import mse as mse\n",
        "from statsmodels.multivariate.pca import PCA\n",
        "\n",
        "import pycuda.autoinit\n",
        "import pycuda.gpuarray as gpuarray\n",
        "import numpy as np\n",
        "import skcuda.linalg as linalg\n",
        "from skcuda.linalg import PCA as cuPCA\n",
        "\n",
        "file_prefix = './data/'\n",
        "\n",
        "### Load the datasets from disk\n",
        "itemCategories=pd.read_csv(file_prefix + 'item_categories.csv')\n",
        "shops=pd.read_csv(file_prefix + 'shops.csv')\n",
        "test=pd.read_csv(file_prefix + 'test.csv')\n",
        "sales_original = pd.read_csv(file_prefix + 'sales_train.csv')\n",
        "\n",
        "items = pd.read_csv(file_prefix + 'items.csv')\n",
        "items = items.drop('item_name', axis = 1)\n",
        "items.set_index('item_id', inplace = True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.3 s, sys: 209 ms, total: 1.51 s\n",
            "Wall time: 2.08 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4kwx0ECH_Cg",
        "colab_type": "text"
      },
      "source": [
        "### 3 Data Cleaning, Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHgA59oIH1O5",
        "colab_type": "code",
        "outputId": "1c7ca179-c51a-47d3-88fa-f82c36ef0b83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "SPAN = 5\n",
        "sales = sales_original.copy()\n",
        "\n",
        "## Separate the cross-validation set\n",
        "## TODO: implement \"rolling\" cross-validation\n",
        "## Filter here so that we are not using the future data\n",
        "sales = sales[sales.date_block_num < 32]\n",
        "\n",
        "## Aggregate prices and sales over months\n",
        "df = sales.groupby(['date_block_num', 'shop_id', 'item_id']).agg(\n",
        "    sold = pd.NamedAgg(column = 'item_cnt_day', aggfunc = 'sum'),\n",
        "    price = pd.NamedAgg(column = 'item_price', aggfunc = 'first')\n",
        ")\n",
        "df.reset_index(drop = False, inplace = True)\n",
        "\n",
        "## Append item_id category variable from the other dataset\n",
        "df.set_index('item_id', drop = True, inplace = True)\n",
        "df = df.join(items, on = 'item_id')\n",
        "df.reset_index(inplace = True, drop = False)\n",
        "\n",
        "\n",
        "### append historic data for up to SPAN months before\n",
        "aggs = df.copy()\n",
        "aggs.drop(inplace = True, columns = ['item_category_id'])\n",
        "aggs.rename(columns = {'date_block_num': 'date_block_before'}, inplace = True)\n",
        "aggs.set_index(\n",
        "    ['item_id', 'shop_id', 'date_block_before'], inplace = True, drop = True)\n",
        "for it in range(1, SPAN + 1):\n",
        "  df['date_block_before'] = df.date_block_num - it\n",
        "  ## Here is where we filter for the first month/s to always have all features\n",
        "  ## (some features are computed back in time, eg. last month's price)\n",
        "  df = df[df.date_block_before >= 0]\n",
        "  df = df.join(\n",
        "      aggs,\n",
        "      on = ['item_id', 'shop_id', 'date_block_before'],\n",
        "      rsuffix = ('_' + str(it))\n",
        "  )\n",
        "  df.drop('date_block_before', axis = 1, inplace = True)\n",
        "\n",
        "  #### Some combinations of shop-item have 0 sales in some months\n",
        "  ## those items that haven't had any sales in the given slots are assigned 0\n",
        "  ## instead of NaN\n",
        "  df['sold_' + str(it)] = df['sold_' + str(it)].fillna(0)\n",
        "  ## If the price wasn't present at the time:\n",
        "  ##   - first, fill backwards in time the last price (works with multiple prices)\n",
        "  ##   - then fill front\n",
        "  df['price_' + str(it)] = df['price_' + str(it)].fillna(method = 'backfill')\n",
        "  df['price_' + str(it)] = df['price_' + str(it)].fillna(method = 'ffill')\n",
        "  df['price_' + str(it) + '_log'] = np.log(df['price_' + str(it)].values)\n",
        "  df = df.drop('price_' + str(it), axis = 1)\n",
        "df['price_log'] = np.log(df['price'].values)\n",
        "#df = df.drop('price', axis = 1)\n",
        "\n",
        "\n",
        "## Can't use any variable that represents time, because it will\n",
        "## memorize the dataset (eg. if it knows the prices are a bit higher in\n",
        "## 2015 compared to 2013 the coeff. will be positive while the prices might fall\n",
        "## again in 2016).\n",
        "## Think about this for the final submission. IRL, using it is lying.\n",
        "df = df.drop(['date_block_num'], axis = 1)\n",
        "df.reset_index(inplace = True, drop = True)\n",
        "\n",
        "\n",
        "## One-hot-encoding for item_id, shop_id, item_category_id\n",
        "## drop_invariant has to be True\n",
        "##    it means the constant columns will be dropped\n",
        "##    this will prevent the normal equations from failing - the matrix won't\n",
        "##    be singular\n",
        "enc = ce.BinaryEncoder(cols=['item_id', 'shop_id', 'item_category_id'],\n",
        "                           drop_invariant = True)\n",
        "enc.fit(df.loc[:, df.columns != 'sold'], df.sold)\n",
        "X_enc = enc.transform(df.loc[:, df.columns != 'sold'])\n",
        "Y_enc = df.sold"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 11.4 s, sys: 671 ms, total: 12.1 s\n",
            "Wall time: 12.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63mVZ9__UvJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = cuPCA(epsilon = 1e-2)\n",
        "X_gpu = gpuarray.GPUArray(X_enc.shape, np.float64, order=\"F\")\n",
        "X_gpu.set(X.values)\n",
        "T_gpu = pca.fit_transform(X_gpu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Onsk9hqOU3C4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = T_gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWb9HugPSiIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "## Model building\n",
        "\n",
        "X = sm.add_constant(X) ## add offset\n",
        "\n",
        "model = sm.OLS(Y, X).fit()\n",
        "predictions = model.predict(X) \n",
        "\n",
        "print_model = model.summary()\n",
        "print(print_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkvUpnC3HOZG",
        "colab_type": "text"
      },
      "source": [
        "### 4 Model Checking - Cross Validation and other\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTKaZLEvSlDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cv_dataset(sales_original, month, span):\n",
        "  cv = sales_original[sales_original.date_block_num == month].copy()\n",
        "  cvdf = cv.groupby(['shop_id', 'item_id']).agg(\n",
        "      sold = pd.NamedAgg(column = 'item_cnt_day', aggfunc = 'sum'),\n",
        "      price = pd.NamedAgg(column = 'item_price', aggfunc = 'first')\n",
        "  )\n",
        "  cvdf.reset_index(drop = False, inplace = True)\n",
        "  cvdf.set_index('item_id', drop = True, inplace = True)\n",
        "  cvdf = cvdf.join(items, on = 'item_id')\n",
        "  cvdf.reset_index(inplace = True, drop = False)\n",
        "  \n",
        "  for it in range(1, span + 1):\n",
        "    prev = sales_original[sales_original.date_block_num == month - it]\n",
        "    pdf = prev.groupby(['shop_id', 'item_id']).agg(\n",
        "        sold = pd.NamedAgg(column = 'item_cnt_day', aggfunc = 'sum'),\n",
        "        price = pd.NamedAgg(column = 'item_price', aggfunc = 'first')\n",
        "    )\n",
        "    cvdf = cvdf.join(pdf, on = ['shop_id', 'item_id'], rsuffix = '_' + str(it))\n",
        "    cvdf['sold_' + str(it)] = cvdf['sold_' + str(it)].fillna(0)\n",
        "    cvdf['price_' + str(it)] = cvdf['price_' + str(it)].fillna(method = 'backfill')\n",
        "    cvdf['price_' + str(it)] = cvdf['price_' + str(it)].fillna(method = 'ffill')\n",
        "    cvdf['price_' + str(it) + '_log' ] = np.log(cvdf['price_' + str(it)].values)\n",
        "    cvdf = cvdf.drop('price_' + str(it), axis = 1)\n",
        "  cvdf['price_log'] = np.log(cvdf['price'].values)\n",
        "  #cvdf = cvdf.drop('price', axis = 1)\n",
        "  return cvdf\n",
        "\n",
        "cvdf = get_cv_dataset(sales_original, 32, SPAN)\n",
        "\n",
        "X_cv = enc.transform(cvdf.loc[:, cvdf.columns != 'sold'])\n",
        "X_cv = sm.add_constant(X_cv) ## add offset\n",
        "Y_cv = cvdf.sold\n",
        "\n",
        "predictions = model.predict(X_cv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_L6gHezQcXO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf89598d-351d-44ef-81cb-c4679865d223"
      },
      "source": [
        "math.sqrt(mse(predictions, Y_cv)), model.mse_total"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17.165870950393856, 78.54915343910456)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cErWmbjNDfd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5d87a6d9-cf9d-4834-bcd2-5b3ce605a9ac"
      },
      "source": [
        "math.sqrt(mse(predictions, Y_cv)), model.mse_total"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17.166284993830292, 78.54915343910456)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9V7Xo5JSpo1",
        "colab_type": "code",
        "outputId": "57667092-3c8b-42de-f24a-1b33298d7995",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "math.sqrt(mse(predictions, Y_cv)), model.mse_total"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17.156509971134057, 73.88989502150442)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZazxAXqES8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#plt.matshow(df.loc[:, df.columns.isin(['price_log', 'price_1_log', 'sold_1', 'sold_2', 'sold_3', 'price_2_log', 'sold'])].corr())\n",
        "#plt.show()\n",
        "\n",
        "sns.pairplot(df)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwh61ecDHZRQ",
        "colab_type": "text"
      },
      "source": [
        "### Extra: Random code bits\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2POnvLxSuZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## don't delete, this unstack idea is top shit and will be needed for sure\n",
        "def agg_months_as_columns(sales):\n",
        "  df['month'] = df.time.dt.month\n",
        "  df = pd.DataFrame(sales.groupby(['month', 'shop_id', 'item_id', \\\n",
        "                                   'item_price'])['item_cnt_day'].sum())\n",
        "  df = df.unstack(level = 0)\n",
        "  df = df.fillna(0)\n",
        "  df.columns = df.columns.get_level_values(1)\n",
        "  df.reset_index(drop = False, inplace = True)\n",
        "  df.columns = ['shop_id', 'item_price','Jan', 'Feb',\\\n",
        "                'Mar',  'Apr', 'May',  'Jun', 'Jul',  \\\n",
        "                'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "  return df\n",
        "\n",
        "### Forward selection with statsmodels\n",
        "def forward_selected(data, response):\n",
        "    \"\"\"Linear model designed by forward selection.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : pandas DataFrame with all possible predictors and response\n",
        "\n",
        "    response: string, name of response column in data\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    model: an \"optimal\" fitted statsmodels linear model\n",
        "           with an intercept\n",
        "           selected by forward selection\n",
        "           evaluated by adjusted R-squared\n",
        "    \"\"\"\n",
        "    remaining = set(data.columns)\n",
        "    remaining.remove(response)\n",
        "    selected = []\n",
        "    current_score, best_new_score = 0.0, 0.0\n",
        "    while remaining and current_score == best_new_score:\n",
        "        scores_with_candidates = []\n",
        "        for candidate in remaining:\n",
        "            formula = \"{} ~ {} + 1\".format(response,\n",
        "                                           ' + '.join(selected + [candidate]))\n",
        "            score = smf.ols(formula, data).fit().rsquared_adj\n",
        "            scores_with_candidates.append((score, candidate))\n",
        "        scores_with_candidates.sort()\n",
        "        best_new_score, best_candidate = scores_with_candidates.pop()\n",
        "        if current_score < best_new_score:\n",
        "            remaining.remove(best_candidate)\n",
        "            selected.append(best_candidate)\n",
        "            current_score = best_new_score\n",
        "    formula = \"{} ~ {} + 1\".format(response,\n",
        "                                   ' + '.join(selected))\n",
        "    model = smf.ols(formula, data).fit()\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}